{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=np.nan)\n",
    "from scipy.stats import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2821: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(984309, 32)\n",
      "(1008911, 32)\n"
     ]
    }
   ],
   "source": [
    "dfa = pd.DataFrame.from_csv(path='/Users/elenahaffmans/Documents/-CogSci3/experiment/data/Group1okt30.csv', sep=',')\n",
    "dfb = pd.DataFrame.from_csv(path='/Users/elenahaffmans/Documents/-CogSci3/experiment/data/Group2nov3.csv', sep=',')\n",
    "\n",
    "#add column group:\n",
    "dfa['Group'] = dfa.index\n",
    "dfb['Group'] = dfb.index\n",
    "\n",
    "print(dfa.shape)\n",
    "print(dfb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we concatenate, the FixationIndex is not unique anymore. \n",
    "So first do the StudioEventData naming and the deleting of first fixations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#for dfa\n",
    "#create new unique indexing\n",
    "indexing = list(range(dfa.shape[0]))\n",
    "dfa = dfa.set_index([indexing])\n",
    "\n",
    "name='empty'\n",
    "row=0\n",
    "for val in dfa.StudioEventData:\n",
    "    if pd.isnull(val):\n",
    "        dfa.set_value(dfa.index[row],'StudioEventData',name)\n",
    "    else:\n",
    "        name= val\n",
    "        #print(name)\n",
    "    row+=1\n",
    "#print(dfa[:40])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ParticipantName StudioEventData  FixationIndex  GazeEventDuration  \\\n",
      "0           PP002           empty            NaN                NaN   \n",
      "1           PP002           empty            1.0              558.0   \n",
      "2           PP002           empty            1.0              558.0   \n",
      "3           PP002           empty            1.0              558.0   \n",
      "4           PP002           empty            1.0              558.0   \n",
      "5           PP002           empty            1.0              558.0   \n",
      "6           PP002           empty            1.0              558.0   \n",
      "7           PP002           empty            1.0              558.0   \n",
      "8           PP002           empty            1.0              558.0   \n",
      "9           PP002           empty            1.0              558.0   \n",
      "\n",
      "   AOI[normal]Hit  AOI[lit]Hit  AOI[lit]Hit.1  AOI[fig]Hit  AOI[normal]Hit.1  \\\n",
      "0             NaN          NaN            NaN          NaN               NaN   \n",
      "1             NaN          NaN            NaN          NaN               NaN   \n",
      "2             NaN          NaN            NaN          NaN               NaN   \n",
      "3             NaN          NaN            NaN          NaN               NaN   \n",
      "4             NaN          NaN            NaN          NaN               NaN   \n",
      "5             NaN          NaN            NaN          NaN               NaN   \n",
      "6             NaN          NaN            NaN          NaN               NaN   \n",
      "7             NaN          NaN            NaN          NaN               NaN   \n",
      "8             NaN          NaN            NaN          NaN               NaN   \n",
      "9             NaN          NaN            NaN          NaN               NaN   \n",
      "\n",
      "   AOI[lit]Hit.2   ...     AOI[fig]Hit.3  AOI[lit]Hit.7  AOI[fig]Hit.4  \\\n",
      "0            NaN   ...               NaN            NaN            NaN   \n",
      "1            NaN   ...               NaN            NaN            NaN   \n",
      "2            NaN   ...               NaN            NaN            NaN   \n",
      "3            NaN   ...               NaN            NaN            NaN   \n",
      "4            NaN   ...               NaN            NaN            NaN   \n",
      "5            NaN   ...               NaN            NaN            NaN   \n",
      "6            NaN   ...               NaN            NaN            NaN   \n",
      "7            NaN   ...               NaN            NaN            NaN   \n",
      "8            NaN   ...               NaN            NaN            NaN   \n",
      "9            NaN   ...               NaN            NaN            NaN   \n",
      "\n",
      "   AOI[fig]Hit.5  AOI[fig]Hit.6  AOI[normal]Hit.8  AOI[fig]Hit.7  \\\n",
      "0            NaN            NaN               NaN            NaN   \n",
      "1            NaN            NaN               NaN            NaN   \n",
      "2            NaN            NaN               NaN            NaN   \n",
      "3            NaN            NaN               NaN            NaN   \n",
      "4            NaN            NaN               NaN            NaN   \n",
      "5            NaN            NaN               NaN            NaN   \n",
      "6            NaN            NaN               NaN            NaN   \n",
      "7            NaN            NaN               NaN            NaN   \n",
      "8            NaN            NaN               NaN            NaN   \n",
      "9            NaN            NaN               NaN            NaN   \n",
      "\n",
      "   AOI[lit]Hit.8  AOI[fig]Hit.8    Group  \n",
      "0            NaN            NaN  group 2  \n",
      "1            NaN            NaN  group 2  \n",
      "2            NaN            NaN  group 2  \n",
      "3            NaN            NaN  group 2  \n",
      "4            NaN            NaN  group 2  \n",
      "5            NaN            NaN  group 2  \n",
      "6            NaN            NaN  group 2  \n",
      "7            NaN            NaN  group 2  \n",
      "8            NaN            NaN  group 2  \n",
      "9            NaN            NaN  group 2  \n",
      "\n",
      "[10 rows x 32 columns]\n"
     ]
    }
   ],
   "source": [
    "#for dfb\n",
    "#create new unique indexing\n",
    "indexing = list(range(dfb.shape[0]))\n",
    "dfb = dfb.set_index([indexing])\n",
    "\n",
    "name='empty'\n",
    "row=0\n",
    "for val in dfb.StudioEventData:\n",
    "    if pd.isnull(val):\n",
    "        dfb.set_value(dfb.index[row],'StudioEventData',name)\n",
    "    else:\n",
    "        name= val\n",
    "        #print(name)\n",
    "    row+=1\n",
    "print(dfb[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete first fixation for each event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#for every new StudioEventData get the first FixationIndex\n",
    "groupeda = dfa.groupby(['StudioEventData'], axis=0)['FixationIndex'].min()\n",
    "\n",
    "#delete all rows where FixationIndex has one of these values.\n",
    "for val in groupeda:\n",
    "    dfa= dfa[dfa.FixationIndex != val]\n",
    "\n",
    "#same for dfb:\n",
    "groupedb = dfb.groupby(['StudioEventData'], axis=0)['FixationIndex'].min()\n",
    "for val in groupedb:\n",
    "    dfb= dfb[dfb.FixationIndex != val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(968099, 32)\n",
      "(987100, 32)\n",
      "(1955199, 32)\n"
     ]
    }
   ],
   "source": [
    "print(dfa.shape)\n",
    "print(dfb.shape)\n",
    "df = pd.concat([dfa,dfb])\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to make sure that there are the same amount of observations in group A and group B for the ANOVA test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first delete all duplicates and events when fixations are not on the AOI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1955199, 32)\n",
      "(58021, 32)\n",
      "shape after adding 3 columns (58021, 35)\n",
      "(58021, 8)\n",
      "amount unique AOI fixations: (1895, 8)\n"
     ]
    }
   ],
   "source": [
    "#delete all rows that are duplicates \n",
    "print(df.shape)\n",
    "df = df.drop_duplicates()\n",
    "print(df.shape)\n",
    "\n",
    "#first put all values together in 3 collumns (fig, lit and norm)\n",
    "df = df.fillna(0)\n",
    "\n",
    "#all data in 3 new column NaN now.\n",
    "df['AOI_lit'] =    df['AOI[lit]Hit'] + df['AOI[lit]Hit.1'] +df['AOI[lit]Hit.2'] +df['AOI[lit]Hit.3'] +df['AOI[lit]Hit.4'] +df['AOI[lit]Hit.5'] +df['AOI[lit]Hit.6'] +df['AOI[lit]Hit.7'] +df['AOI[lit]Hit.8'] \n",
    "df['AOI_fig'] =    df['AOI[fig]Hit'] + df['AOI[fig]Hit.1'] +df['AOI[fig]Hit.2'] +df['AOI[fig]Hit.3'] +df['AOI[fig]Hit.4'] +df['AOI[fig]Hit.5'] +df['AOI[fig]Hit.6'] +df['AOI[fig]Hit.7'] +df['AOI[fig]Hit.8']\n",
    "df['AOI_normal'] = df['AOI[normal]Hit'] + df['AOI[normal]Hit.1'] +df['AOI[normal]Hit.2'] +df['AOI[normal]Hit.3'] +df['AOI[normal]Hit.4'] +df['AOI[normal]Hit.5'] +df['AOI[normal]Hit.6'] +df['AOI[normal]Hit.7'] +df['AOI[normal]Hit.8']\n",
    "\n",
    "print('shape after adding 3 columns',df.shape)\n",
    "\n",
    "#delete all other columns\n",
    "df = df.drop(['AOI[fig]Hit','AOI[normal]Hit', 'AOI[lit]Hit', 'AOI[lit]Hit.1', 'AOI[normal]Hit.1',\n",
    " 'AOI[fig]Hit.1' ,'AOI[lit]Hit.2' ,'AOI[lit]Hit.3', 'AOI[normal]Hit.2',\n",
    " 'AOI[fig]Hit.2', 'AOI[normal]Hit.3', 'AOI[fig]Hit.3' ,'AOI[normal]Hit.4',\n",
    " 'AOI[fig]Hit.4', 'AOI[lit]Hit.4', 'AOI[normal]Hit.5', 'AOI[normal]Hit.6',\n",
    " 'AOI[fig]Hit.5', 'AOI[normal]Hit.7', 'AOI[lit]Hit.5', 'AOI[lit]Hit.6',\n",
    " 'AOI[fig]Hit.6', 'AOI[fig]Hit.7', 'AOI[lit]Hit.7', 'AOI[normal]Hit.8',\n",
    " 'AOI[fig]Hit.8' ,'AOI[lit]Hit.8'], axis=1)\n",
    "\n",
    "print(df.shape)\n",
    "\n",
    "#Drop all rows where df[\"AOI_lit\"], df[\"AOI_fig\"] and df[\"AOI_normal\"] all are NaN.\n",
    "#create new column with all AOI looks combined, and delete when this column has Nan entry.\n",
    "df[\"Any_AOI\"]= df['AOI_lit']+ df['AOI_fig']+ df['AOI_normal']\n",
    "df = df.replace(0,np.nan)\n",
    "df = df.dropna(axis=0, subset=['Any_AOI'])\n",
    "df = df.drop(['Any_AOI'], axis=1)\n",
    "\n",
    "print('amount unique AOI fixations:', df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now look at what participants are in there, and look at how many Events per group (these have to be the same).\n",
    "We see that 3 questions are missing, 2 in G1 and 1 in G2. Therefor we delete observations in the other categories of the same idiom. We are left with  123 observations per cat in stead of 126."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PP003' 'PP006' 'PP008' 'PP010' 'p012' 'PP014' 'pp017' 'PP002' 'PP004'\n",
      " 'PP007' 'pp009' 'PP011' 'PP013' 'pp018']\n"
     ]
    }
   ],
   "source": [
    "df2=df\n",
    "print(df2['ParticipantName'].unique())\n",
    "\n",
    "#create column wirh group info\n",
    "grouped = df2.groupby(['Group','StudioEventData','ParticipantName'], axis=0)['StudioEventData'].describe() \n",
    "\n",
    "#For missing value G1 question1 PP003 (figurative)\n",
    "df2 = df2[~((df2.ParticipantName =='PP002') & (df2.StudioEventData=='all texts no questions.2_1'))]#G2\n",
    "df2 = df2[~((df2.ParticipantName=='PP006') & (df2.StudioEventData=='all texts no questions.3_1'))]#G1\n",
    "    \n",
    "#For missing value G1 question 27 PP014 (normal)\n",
    "df2 = df2[~((df2.ParticipantName=='PP008') & (df2.StudioEventData=='all texts no questions.25_1'))]#G1\n",
    "df2 = df2[~((df2.ParticipantName=='PP011') & (df2.StudioEventData=='all texts no questions.26_1'))]#G2\n",
    "\n",
    "#For missing value G2 question 44 PP004 (literal)\n",
    "df2 = df2[~((df2.ParticipantName=='p012') & (df2.StudioEventData=='all texts no questions.43_1'))]#G1\n",
    "df2 = df2[~((df2.ParticipantName=='pp018') & (df2.StudioEventData=='all texts no questions.45_1'))]#G2\n",
    "\n",
    "#create column wirh group info\n",
    "grouped = df2.groupby(['Group','StudioEventData','ParticipantName'], axis=0)['StudioEventData'].describe() \n",
    "\n",
    "#pretty print ALL results\n",
    "#with pd.option_context('display.max_rows', None, 'display.max_columns', 3):\n",
    "#    print(grouped)\n",
    "\n",
    "df=df2\n",
    "\n",
    "#pd.DataFrame.to_csv(df,'/Users/elenahaffmans/Documents/-CogSci3/experiment/data/all_processed.csv')\n",
    "#HEt lijkt erop dat groep 2 nu een obseratie meer heeft."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total reading time p literal idiom in ms= 1279.92682927\n",
      "literal std= 851.019705625\n",
      "total reading time p figurative idiom in ms= 1484.07317073\n",
      "figurative std= 1233.73619988\n",
      "total reading time p normal idiom in ms= 1613.85365854\n",
      "literal norm= 1050.85399791\n",
      "Variance for all data together: 1104294.12493\n",
      "\n",
      " 3.0954072273 0.0464434584162\n",
      "  Multiple Comparison of Means - Tukey HSD,FWER=0.05  \n",
      "======================================================\n",
      "  group1    group2  meandiff   lower    upper   reject\n",
      "------------------------------------------------------\n",
      "figurative literal -204.1463  -522.59  114.2973 False \n",
      "figurative  normal  129.7805 -188.6631 448.2241 False \n",
      " literal    normal  333.9268  15.4832  652.3705  True \n",
      "------------------------------------------------------\n",
      "['figurative' 'literal' 'normal']\n"
     ]
    }
   ],
   "source": [
    "### FOR TOTAL READING TIME ###\n",
    "\n",
    "#get df with only lit AOIs\n",
    "lit = df[df['AOI_lit'] == 1]\n",
    "\n",
    "#group by story(page) an pp and get total gaze per story, per person\n",
    "group_lit = lit.groupby(['ParticipantName','StudioEventData'], axis=0)['GazeEventDuration'].sum()\n",
    "gaze_p_story_lit = group_lit.values\n",
    "\n",
    "#14 pp who read 9 literal idioms = 126 observations, but turns out exactly one missing everywhere, so 125 of each cat.\n",
    "print( 'total reading time p literal idiom in ms=', gaze_p_story_lit.sum()/(123)) \n",
    "print('literal std=', gaze_p_story_lit.std())\n",
    "\n",
    "#Same for fig and normal:\n",
    "\n",
    "fig = df[df['AOI_fig'] == 1]\n",
    "\n",
    "#per event, maar nog niet per pp:\n",
    "group_fig = fig.groupby(['ParticipantName','StudioEventData'], axis=0)['GazeEventDuration'].sum() \n",
    "gaze_p_story_fig = group_fig.values\n",
    "\n",
    "#REALLY BAD BUT ADD ONE VALUE BECAUSE ONE OBSERVATION FROM THAT LIST IS MISSING!\n",
    "#gaze_p_story_fig = np.append(gaze_p_story_fig, 1627)\n",
    "print('total reading time p figurative idiom in ms=', gaze_p_story_fig.sum()/(123))\n",
    "\n",
    "print('figurative std=', gaze_p_story_fig.std())\n",
    "\n",
    "norm = df[df['AOI_normal'] == 1]\n",
    "group_norm = norm.groupby(['ParticipantName','StudioEventData'], axis=0)['GazeEventDuration'].sum()\n",
    "gaze_p_story_norm = group_norm.values\n",
    "\n",
    "print( 'total reading time p normal idiom in ms=', gaze_p_story_norm.sum()/(123))\n",
    "print('literal norm=', gaze_p_story_norm.std())\n",
    "\n",
    "'''want to do multivariate repeated Anova measure, but only one way Anova as function\n",
    "Try it for fixation count and total readig time separate now.'''\n",
    "\n",
    "#total variance to use for ETA squared.\n",
    "list_all = list(gaze_p_story_norm) + list(gaze_p_story_norm) + list(gaze_p_story_norm)\n",
    "variance_all = np.var(list_all)\n",
    "print('Variance for all data together:', variance_all)\n",
    "\n",
    "#ANOVA for total readting time:\n",
    "\n",
    "df_p_story = pd.DataFrame({'lit':gaze_p_story_lit,'fig':gaze_p_story_fig,'normal':gaze_p_story_norm })\n",
    "#print(df_p_story.head())\n",
    "f_val, p_val = stats.f_oneway(df_p_story.lit, df_p_story.fig, df_p_story.normal)\n",
    "\n",
    "print('\\n',f_val, p_val)\n",
    "\n",
    "# Turkey \n",
    "\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "from statsmodels.stats.multicomp import MultiComparison\n",
    "\n",
    "#make a list with only data and one with labels.\n",
    "values = np.append(gaze_p_story_lit, [gaze_p_story_fig, gaze_p_story_norm])\n",
    "\n",
    "labellit = ['literal' for x in np.array(range(123))]\n",
    "labelfig = ['figurative' for x in np.array(range(123))]\n",
    "labelnorm = ['normal' for x in np.array(range(123))]\n",
    "labels = np.append(labellit, [labelfig, labelnorm])\n",
    "\n",
    "mc = MultiComparison(values, labels)\n",
    "result = mc.tukeyhsd()\n",
    " \n",
    "print(result)\n",
    "print(mc.groupsunique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lit= 4.48780487805\n",
      "lit std= 2.04170283489\n",
      "fig= 5.0081300813\n",
      "fig std= 3.40132909211\n",
      "norm= 5.69105691057\n",
      "norm std= 3.02867080588\n",
      "Variance for all data together: 8.54625039475\n",
      "   fig  lit  normal\n",
      "0    4    3       4\n",
      "1    2    2       5\n",
      "2    3    8       3\n",
      "3    4    3       4\n",
      "4    4    2       5\n",
      "5.35042852973 0.00512459216677\n",
      "123\n",
      "123\n",
      "123\n",
      "Multiple Comparison of Means - Tukey HSD,FWER=0.05\n",
      "=================================================\n",
      "  group1    group2 meandiff  lower  upper  reject\n",
      "-------------------------------------------------\n",
      "figurative literal -0.5203  -1.3886 0.348  False \n",
      "figurative  normal  0.6829  -0.1854 1.5512 False \n",
      " literal    normal  1.2033   0.335  2.0715  True \n",
      "-------------------------------------------------\n",
      "['figurative' 'literal' 'normal']\n"
     ]
    }
   ],
   "source": [
    "### FOR FIXATION COUNT ### \n",
    "\n",
    "#get df with only lit AOIs\n",
    "lit = df[df['AOI_lit'] == 1]\n",
    "\n",
    "#now we want to count how many unique fixations there where.\n",
    "group_lit1 = lit.groupby(['StudioEventData','ParticipantName'], axis=0)['FixationIndex']\n",
    "tot_fix_lit = group_lit1.count().values\n",
    "print('lit=', tot_fix_lit.sum()/123)\n",
    "print('lit std=', tot_fix_lit.std())\n",
    "\n",
    "\n",
    "group_fig1 = fig.groupby(['StudioEventData','ParticipantName'], axis=0)['FixationIndex']\n",
    "tot_fix_fig = group_fig1.count().values\n",
    "\n",
    "#AGAIN ADDING THE AVERAGE TO THE LIST: 6\n",
    "#tot_fix_fig = np.append(tot_fix_fig, 6)\n",
    "print('fig=', tot_fix_fig.sum()/123)\n",
    "print('fig std=', tot_fix_fig.std())\n",
    "norm1 = df[df['AOI_normal'] == 1] # for some reason otherwise error\n",
    "\n",
    "group_norm1 = norm.groupby(['StudioEventData','ParticipantName'], axis=0)['FixationIndex']\n",
    "tot_fix_norm = group_norm1.count().values\n",
    "print('norm=', tot_fix_norm.sum()/123)\n",
    "print('norm std=', tot_fix_norm.std())\n",
    "\n",
    "#variance for eta squared:\n",
    "all_list = list(tot_fix_norm)+list(tot_fix_lit)+list(tot_fix_fig)\n",
    "print('Variance for all data together:', np.var(all_list))\n",
    "\n",
    "## ANOVA\n",
    "df_fixp_story = pd.DataFrame({'lit':tot_fix_lit,'fig':tot_fix_fig,'normal':tot_fix_norm })\n",
    "print(df_fixp_story.head())\n",
    "f_val, p_val = stats.f_oneway(df_fixp_story.lit, df_fixp_story.fig, df_fixp_story.normal)\n",
    "print(f_val, p_val)\n",
    "\n",
    "\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "from statsmodels.stats.multicomp import MultiComparison\n",
    "\n",
    "#make a list with only data and one with labels.\n",
    "values = np.append(tot_fix_lit,[tot_fix_fig, tot_fix_norm])\n",
    "print(len(tot_fix_lit))\n",
    "print(len(tot_fix_fig))\n",
    "print(len(tot_fix_norm))\n",
    "\n",
    "labellit = ['literal' for x in np.array(range(123))]\n",
    "labelfig = ['figurative' for x in np.array(range(123))]\n",
    "labelnorm = ['normal' for x in np.array(range(123))]\n",
    "labels = np.append(labellit, [labelfig, labelnorm])\n",
    "\n",
    "mc = MultiComparison(values, labels)\n",
    "result = mc.tukeyhsd()\n",
    " \n",
    "print(result)\n",
    "print(mc.groupsunique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "literal=  811.617886179\n",
      "lit std= 651.759675898\n",
      "figuratve =  891.634146341\n",
      "fig std= 716.072329865\n",
      "normal = 878.18699187\n",
      "norm std= 696.18878093\n",
      "Variance for all data together: 475300.233327\n",
      "      fig     lit  normal\n",
      "0   316.0  1499.0   525.0\n",
      "1   400.0   250.0   208.0\n",
      "2  1041.0   874.0   774.0\n",
      "3   308.0   325.0  1024.0\n",
      "4   192.0   691.0   191.0\n",
      "0.472432540213 0.623863445357\n"
     ]
    }
   ],
   "source": [
    "## FOR INITIAL READING TIME ##\n",
    "\n",
    "group_lit2 = lit.groupby(['ParticipantName','StudioEventData'], axis=0)#['FixationIndex','GazeEventDuration']\n",
    "\n",
    "'''for each AOI sum the gaze time as long as the fixation index de vorige opvolgt(i.e. niet weggekeken)'''\n",
    "lit_initial = []\n",
    "for (fixIndex, gaze), group in group_lit2['FixationIndex', 'GazeEventDuration']:\n",
    "    idx=0\n",
    "    for fix in group['FixationIndex']:\n",
    "        if fix == group['FixationIndex'].min(): #fix is fixation index\n",
    "            count = group['GazeEventDuration'].iloc[0] \n",
    "            idx+=1\n",
    "            fix_idx = fix\n",
    "        elif fix-1 == fix_idx:\n",
    "            count += group['GazeEventDuration'].iloc[idx]\n",
    "            fix_idx = fix\n",
    "            idx=idx+1\n",
    "        else:\n",
    "            break\n",
    "    lit_initial.append(count) #now append initial time to a list of all observations\n",
    "\n",
    "print('literal= ', np.average(np.asarray(lit_initial)))\n",
    "print('lit std=',(np.asarray(lit_initial)).std())\n",
    "\n",
    "#same for fig and normal:\n",
    "\n",
    "group_fig2 = fig.groupby(['ParticipantName','StudioEventData'], axis=0)#['FixationIndex','GazeEventDuration']\n",
    "\n",
    "'''for each AOI sum the gaze time as long as the fixation index de vorige opvolgt(i.e. niet weggekeken)'''\n",
    "fig_initial = []\n",
    "for (fixIndex, gaze), group in group_fig2['FixationIndex', 'GazeEventDuration']:\n",
    "    idx=0\n",
    "    for fix in group['FixationIndex']:\n",
    "        if fix == group['FixationIndex'].min(): #fix is fixation index\n",
    "            count = group['GazeEventDuration'].iloc[0] \n",
    "            idx+=1\n",
    "            fix_idx = fix\n",
    "        elif fix-1 == fix_idx:\n",
    "            count += group['GazeEventDuration'].iloc[idx]\n",
    "            fix_idx = fix\n",
    "            idx=idx+1\n",
    "        else:\n",
    "            break\n",
    "    fig_initial.append(count) #now append initial time to a list of all observations\n",
    "\n",
    "print('figuratve = ', np.average(np.asarray(fig_initial)))\n",
    "print('fig std=',(np.asarray(fig_initial)).std())\n",
    "\n",
    "\n",
    "group_norm2 = norm.groupby(['ParticipantName','StudioEventData'], axis=0)#['FixationIndex','GazeEventDuration']\n",
    "\n",
    "'''for each AOI sum the gaze time as long as the fixation index de vorige opvolgt(i.e. niet weggekeken)'''\n",
    "norm_initial = []\n",
    "for (fixIndex, gaze), group in group_norm2['FixationIndex', 'GazeEventDuration']:\n",
    "    idx=0\n",
    "    for fix in group['FixationIndex']:\n",
    "        if fix == group['FixationIndex'].min(): #fix is fixation index\n",
    "            count = group['GazeEventDuration'].iloc[0] \n",
    "            idx+=1\n",
    "            fix_idx = fix\n",
    "        elif fix-1 == fix_idx:\n",
    "            count += group['GazeEventDuration'].iloc[idx]\n",
    "            fix_idx = fix\n",
    "            idx=idx+1\n",
    "        else:\n",
    "            break\n",
    "    norm_initial.append(count) #now append initial time to a list of all observations\n",
    "\n",
    "print('normal =', np.average(np.asarray(norm_initial)))\n",
    "print('norm std=',(np.asarray(norm_initial)).std())\n",
    "\n",
    "#Vairance of all for callculation ETA squared done in excel.\n",
    "variance_all = np.var(np.array(norm_initial + lit_initial + fig_initial))\n",
    "print('Variance for all data together:',variance_all)\n",
    "\n",
    "## ANOVA\n",
    "df_init_story = pd.DataFrame({'lit':lit_initial,'fig':fig_initial,'normal':norm_initial })\n",
    "print(df_init_story.head())\n",
    "f_val, p_val = stats.f_oneway(df_init_story.lit, df_init_story.fig, df_init_story.normal)\n",
    "print(f_val, p_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "PART II:\n",
    "POST HOC TESTING - \n",
    "Figurative IDIOMS WITH 1 and 2 VERSUS 3 and 4 SCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        FixationIndex  GazeEventDuration    Group ParticipantName  \\\n",
      "472312         3406.0              258.0  group 2           PP007   \n",
      "472343         3407.0              425.0  group 2           PP007   \n",
      "\n",
      "                    StudioEventData  AOI_lit  AOI_fig  AOI_normal  \n",
      "472312  all texts no questions.34_1      NaN      1.0         NaN  \n",
      "472343  all texts no questions.34_1      NaN      1.0         NaN  \n"
     ]
    }
   ],
   "source": [
    "##G1: ['PP003' 'PP006' 'PP008' 'PP010' 'p012' 'PP014' 'pp017']\n",
    "##G2: ['PP002' 'PP004' 'PP007' 'pp009' 'PP011' 'PP013' 'pp018']\n",
    "\n",
    "df3 = df\n",
    "\n",
    "#create column wirh group info\n",
    "grouped = df3.groupby(['Group','StudioEventData','ParticipantName'], axis=0)['StudioEventData'].describe() \n",
    "#pretty print ALL results\n",
    "#with pd.option_context('display.max_rows', None, 'display.max_columns', 3):\n",
    "#    print(grouped)\n",
    "    \n",
    "#correct figurative sentences rating 1\n",
    "a = df3[((df3.ParticipantName =='PP011') & (df3.StudioEventData=='all texts no questions.4_1'))]\n",
    "b = df3[((df3.ParticipantName =='pp009') & (df3.StudioEventData=='all texts no questions.16_1'))]\n",
    "c = df3[((df3.ParticipantName =='pp017') & (df3.StudioEventData=='all texts no questions.19_1'))]\n",
    "d = df3[((df3.ParticipantName =='PP013') & (df3.StudioEventData=='all texts no questions.34_1'))]\n",
    "e = df3[((df3.ParticipantName =='PP004') & (df3.StudioEventData=='all texts no questions.40_1'))]\n",
    "f = df3[((df3.ParticipantName =='pp009') & (df3.StudioEventData=='all texts no questions.40_1'))]\n",
    "g = df3[((df3.ParticipantName =='PP011') & (df3.StudioEventData=='all texts no questions.46_1'))]\n",
    "\n",
    "#rating 2\n",
    "aa = df3[((df3.ParticipantName =='pp009') & (df3.StudioEventData=='all texts no questions.10_1'))]\n",
    "bb = df3[((df3.ParticipantName =='PP002') & (df3.StudioEventData=='all texts no questions.16_1'))]\n",
    "cc = df3[((df3.ParticipantName =='PP002') & (df3.StudioEventData=='all texts no questions.22_1'))]\n",
    "dd = df3[((df3.ParticipantName =='PP014') & (df3.StudioEventData=='all texts no questions.31_1'))]\n",
    "ee = df3[((df3.ParticipantName =='pp009') & (df3.StudioEventData=='all texts no questions.34_1'))]\n",
    "ff = df3[((df3.ParticipantName =='PP011') & (df3.StudioEventData=='all texts no questions.34_1'))]\n",
    "gg = df3[((df3.ParticipantName =='pp018') & (df3.StudioEventData=='all texts no questions.40_1'))]\n",
    "hh = df3[((df3.ParticipantName =='pp018') & (df3.StudioEventData=='all texts no questions.52_1'))]\n",
    "#concatenate these df's\n",
    "df_1 = pd.concat([a,b,c,d,e,f,g,aa,bb,cc,dd,ee,ff,gg,hh])\n",
    "\n",
    "#matching 1's\n",
    "i = df3[((df3.ParticipantName =='pp018') & (df3.StudioEventData=='all texts no questions.4_1'))]\n",
    "j = df3[((df3.ParticipantName =='PP013') & (df3.StudioEventData=='all texts no questions.16_1'))]\n",
    "k = df3[((df3.ParticipantName =='PP014') & (df3.StudioEventData=='all texts no questions.19_1'))]\n",
    "l = df3[((df3.ParticipantName =='PP004') & (df3.StudioEventData=='all texts no questions.34_1'))]\n",
    "m = df3[((df3.ParticipantName =='PP002') & (df3.StudioEventData=='all texts no questions.40_1'))]\n",
    "n = df3[((df3.ParticipantName =='pp018') & (df3.StudioEventData=='all texts no questions.46_1'))]\n",
    "o = df3[((df3.ParticipantName =='PP007') & (df3.StudioEventData=='all texts no questions.40_1'))]\n",
    "\n",
    "#matching rating 2\n",
    "ii = df3[((df3.ParticipantName =='PP004') & (df3.StudioEventData=='all texts no questions.10_1'))]\n",
    "jj = df3[((df3.ParticipantName =='PP007') & (df3.StudioEventData=='all texts no questions.16_1'))]\n",
    "kk = df3[((df3.ParticipantName =='pp009') & (df3.StudioEventData=='all texts no questions.22_1'))]\n",
    "ll = df3[((df3.ParticipantName =='PP006') & (df3.StudioEventData=='all texts no questions.31_1'))]\n",
    "mm = df3[((df3.ParticipantName =='PP007') & (df3.StudioEventData=='all texts no questions.34_1'))]\n",
    "nn = df3[((df3.ParticipantName =='pp018') & (df3.StudioEventData=='all texts no questions.34_1'))]\n",
    "oo = df3[((df3.ParticipantName =='PP013') & (df3.StudioEventData=='all texts no questions.40_1'))]\n",
    "pp = df3[((df3.ParticipantName =='PP013') & (df3.StudioEventData=='all texts no questions.52_1'))]\n",
    "\n",
    "df_4 = pd.concat([i,j,k,l,m,n,o,ii,jj,kk,ll,mm,nn,oo,pp])\n",
    "print(mm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1932.  1831.  5111.  3781.  1782.  1408.  4973.   433.   466.   658.\n",
      "  1273.  3630.  1506.  1215.  1132.]\n",
      "[ 6461.  1108.  3224.   433.   782.   683.  1957.  1924.  2647.  6262.\n",
      "   774.   658.  2973.  2523.   566.]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'scipy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-2435c03620dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgaze_p_story_4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mttest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mttest_ind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgaze_p_story_1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgaze_p_story_4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mttest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m##Total fixation count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'scipy' is not defined"
     ]
    }
   ],
   "source": [
    "##TOtal reading time\n",
    "from scipy import stats\n",
    "\n",
    "#group by story(page) an pp and get total gaze per story, per person\n",
    "group_1 = df_1.groupby(['ParticipantName','StudioEventData'], axis=0)['GazeEventDuration'].sum()\n",
    "gaze_p_story_1 = group_1.values\n",
    "print(gaze_p_story_1)\n",
    "\n",
    "group_4 = df_4.groupby(['ParticipantName','StudioEventData'], axis=0)['GazeEventDuration'].sum()\n",
    "gaze_p_story_4 = group_4.values\n",
    "print(gaze_p_story_4)\n",
    "\n",
    "ttest = scipy.stats.ttest_ind(gaze_p_story_1,gaze_p_story_4)\n",
    "print(ttest)\n",
    "##Total fixation count\n",
    "\n",
    "group_11 = df_1.groupby(['StudioEventData','ParticipantName'], axis=0)['FixationIndex']\n",
    "tot_fix_1 = group_11.count().values\n",
    "print('G1=', tot_fix_1.sum()/5)\n",
    "group_44 = df_4.groupby(['StudioEventData','ParticipantName'], axis=0)['FixationIndex']\n",
    "tot_fix_4 = group_44.count().values\n",
    "print('G4=', tot_fix_4.sum()/5)\n",
    "\n",
    "print(scipy.stats.ttest_ind(tot_fix_1,tot_fix_4))\n",
    "\n",
    "##For initial reading time\n",
    "\n",
    "group_111 = df_1.groupby(['ParticipantName','StudioEventData'], axis=0)#['FixationIndex','GazeEventDuration']\n",
    "\n",
    "'''for each AOI sum the gaze time as long as the fixation index de vorige opvolgt(i.e. niet weggekeken)'''\n",
    "G1_initial = []\n",
    "for (fixIndex, gaze), group in group_111['FixationIndex', 'GazeEventDuration']:\n",
    "    idx=0\n",
    "    for fix in group['FixationIndex']:\n",
    "        if fix == group['FixationIndex'].min(): #fix is fixation index\n",
    "            count = group['GazeEventDuration'].iloc[0] \n",
    "            idx+=1\n",
    "            fix_idx = fix\n",
    "        elif fix-1 == fix_idx:\n",
    "            count += group['GazeEventDuration'].iloc[idx]\n",
    "            fix_idx = fix\n",
    "            idx=idx+1\n",
    "        else:\n",
    "            break\n",
    "    G1_initial.append(count) #now append initial time to a list of all observations\n",
    "\n",
    "print(G1_initial)\n",
    "\n",
    "group_444 = df_4.groupby(['ParticipantName','StudioEventData'], axis=0)#['FixationIndex','GazeEventDuration']\n",
    "\n",
    "'''for each AOI sum the gaze time as long as the fixation index de vorige opvolgt(i.e. niet weggekeken)'''\n",
    "G4_initial = []\n",
    "for (fixIndex, gaze), group in group_444['FixationIndex', 'GazeEventDuration']:\n",
    "    idx=0\n",
    "    for fix in group['FixationIndex']:\n",
    "        if fix == group['FixationIndex'].min(): #fix is fixation index\n",
    "            count = group['GazeEventDuration'].iloc[0] \n",
    "            idx+=1\n",
    "            fix_idx = fix\n",
    "        elif fix-1 == fix_idx:\n",
    "            count += group['GazeEventDuration'].iloc[idx]\n",
    "            fix_idx = fix\n",
    "            idx=idx+1\n",
    "        else:\n",
    "            break\n",
    "    G4_initial.append(count) #now append initial time to a list of all observations\n",
    "\n",
    "print(G4_initial)\n",
    "\n",
    "\n",
    "print(scipy.stats.ttest_ind(tot_fix_1,tot_fix_4))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
